{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE_FOLDER = \"/workspaces/valuation/data\"\n",
    "NUMPY_DATA_DESTINATION = \"/workspaces/valuation/data/staging/numpy\"\n",
    "OUTPUT_FILENAME = \"dfp.duckdb\"\n",
    "SOURCE_TABLE = \"gold_dfp_dre_pivoted\"\n",
    "\n",
    "# How many periods to look back (rows in the past).\n",
    "TIME_STEPS = 3\n",
    "\n",
    "# How many last records to keep as test data.\n",
    "TEST_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_path = os.path.join(DATA_SOURCE_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Create or connect to the DuckDB database\n",
    "conn = duckdb.connect(database=db_path, read_only=False)\n",
    "\n",
    "# Read data\n",
    "df = conn.sql(f\"SELECT * FROM {SOURCE_TABLE}\").fetchdf()\n",
    "\n",
    "# Close the DuckDB connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DIA_REFER\"] = df[\"DT_REFER\"].dt.day\n",
    "df[\"MES_REFER\"] = df[\"DT_REFER\"].dt.month   \n",
    "df[\"ANO_REFER\"] = df[\"DT_REFER\"].dt.year\n",
    "\n",
    "# List of parameter, target must be the first one.\n",
    "FEATURE_NAMES = ['RECEITA', 'EBIT', 'LAIR', 'PERIODO_MESES', 'DIA_REFER', 'MES_REFER', 'ANO_REFER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the time steps (lookback)\n",
    "\n",
    "The shape must be:\n",
    "(samples, time steps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date if necessary\n",
    "df.sort_values(by=['CD_CVM', 'DT_REFER'], inplace=True)\n",
    "\n",
    "# Create sequences for each CD_CVM group\n",
    "grouped = df.groupby('CD_CVM')\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for name, group in grouped:\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "        \n",
    "    # Select the necessary columns\n",
    "    data = group[FEATURE_NAMES].values\n",
    "        \n",
    "    # Create the sequences\n",
    "    for i in range(TIME_STEPS, len(data)):\n",
    "        X_list.append(data[i-TIME_STEPS:i])\n",
    "        y_list.append(data[i, 0])\n",
    "\n",
    "    # Split into train and test\n",
    "    split_index = len(X_list) - TEST_SIZE\n",
    "    X_train_list.extend(X_list[:split_index])\n",
    "    y_train_list.extend(y_list[:split_index])\n",
    "    X_test_list.extend(X_list[split_index:])\n",
    "    y_test_list.extend(y_list[split_index:])\n",
    "    \n",
    "# Convert lists to numpy arrays\n",
    "X_train, y_train = np.array(X_train_list), np.array(y_train_list)\n",
    "X_test, y_test = np.array(X_test_list), np.array(y_test_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# Fit the scaler on the training data and transform both train and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save numpy arrays to disk\n",
    "if not os.path.exists(NUMPY_DATA_DESTINATION):\n",
    "    os.makedirs(NUMPY_DATA_DESTINATION)\n",
    "    \n",
    "np.save(os.path.join(NUMPY_DATA_DESTINATION, 'X_train.npy'), X_train_scaled)\n",
    "np.save(os.path.join(NUMPY_DATA_DESTINATION, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(NUMPY_DATA_DESTINATION, 'X_test.npy'), X_test_scaled)\n",
    "np.save(os.path.join(NUMPY_DATA_DESTINATION, 'y_test.npy'), y_test)\n",
    "\n",
    "# Print shapes to verify the split and scaling\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
