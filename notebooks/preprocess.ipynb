{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import numpy as np\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE_FOLDER = \"/workspaces/valuation/data\"\n",
    "NUMPY_DATA_DESTINATION = \"/workspaces/valuation/data/staging/numpy\"\n",
    "OUTPUT_FILENAME = \"dfp.duckdb\"\n",
    "SOURCE_TABLE = \"gold_dfp_dre_pivoted\"\n",
    "\n",
    "# How many periods to look back (rows in the past).\n",
    "TIME_STEPS = 3\n",
    "\n",
    "# How many last records to keep as test data.\n",
    "TEST_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserException",
     "evalue": "Parser Error: syntax error at end of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m conn \u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mconnect(database\u001b[38;5;241m=\u001b[39mdb_path, read_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read data\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSOURCE_TABLE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m WHERE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchdf()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Close the DuckDB connection\u001b[39;00m\n\u001b[1;32m     10\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mParserException\u001b[0m: Parser Error: syntax error at end of input"
     ]
    }
   ],
   "source": [
    "\n",
    "db_path = os.path.join(DATA_SOURCE_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "# Create or connect to the DuckDB database\n",
    "conn = duckdb.connect(database=db_path, read_only=False)\n",
    "\n",
    "# Read data\n",
    "df = conn.sql(f\"SELECT * FROM {SOURCE_TABLE}\").fetchdf()\n",
    "\n",
    "# Close the DuckDB connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Nulls\n",
    "\n",
    "Remove rows with at least one null value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProfileReport(df, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DIA_REFER\"] = df[\"DT_REFER\"].dt.day\n",
    "df[\"MES_REFER\"] = df[\"DT_REFER\"].dt.month   \n",
    "df[\"ANO_REFER\"] = df[\"DT_REFER\"].dt.year\n",
    "\n",
    "# List of parameter, target must be the first one.\n",
    "FEATURE_NAMES = ['RECEITA', 'EBIT', 'LAIR', 'PERIODO_MESES', 'DIA_REFER', 'MES_REFER', 'ANO_REFER']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the time steps (lookback)\n",
    "\n",
    "The shape must be:\n",
    "(samples, time steps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date if necessary\n",
    "df.sort_values(by=['CD_CVM', 'DT_REFER'], inplace=True)\n",
    "\n",
    "# Create sequences for each CD_CVM group\n",
    "grouped = df.groupby('CD_CVM')\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list, y_test_list = [], []\n",
    "\n",
    "for name, group in grouped:\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "        \n",
    "    # Select the necessary columns\n",
    "    data = group[FEATURE_NAMES].values\n",
    "        \n",
    "    # Create the sequences\n",
    "    for i in range(TIME_STEPS, len(data)):\n",
    "        X_list.append(data[i-TIME_STEPS:i])\n",
    "        y_list.append(data[i, 0])\n",
    "\n",
    "    # Split into train and test\n",
    "    split_index = len(X_list) - TEST_SIZE\n",
    "    X_train_list.extend(X_list[:split_index])\n",
    "    y_train_list.extend(y_list[:split_index])\n",
    "    X_test_list.extend(X_list[split_index:])\n",
    "    y_test_list.extend(y_list[split_index:])\n",
    "    \n",
    "# Convert lists to numpy arrays\n",
    "X_train, y_train = np.array(X_train_list, dtype=np.float32), np.array(y_train_list, dtype=np.float32)\n",
    "X_test, y_test = np.array(X_test_list, dtype=np.float32), np.array(y_test_list, dtype=np.float32)\n",
    "\n",
    "# Reshape the targets from (n,) to (n,1)\n",
    "y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "y_test = np.reshape(y_test, (len(y_test), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler\n",
    "#scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# Fit the scaler on the training data and transform both train and test data\n",
    "#X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "#X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save numpy arrays to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (6510, 3, 6)\n",
      "X_test_scaled shape: (1027, 3, 6)\n",
      "y_train shape: (6510, 1)\n",
      "y_test shape: (1027, 1)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(NUMPY_DATA_DESTINATION):\n",
    "    os.makedirs(NUMPY_DATA_DESTINATION)\n",
    "    \n",
    "np.save(os.path.join(NUMPY_DATA_DESTINATION, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(NUMPY_DATA_DESTINATION, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(NUMPY_DATA_DESTINATION, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(NUMPY_DATA_DESTINATION, 'y_test.npy'), y_test)\n",
    "\n",
    "# Print shapes to verify the split and scaling\n",
    "print(\"X_train_scaled shape:\", X_train.shape)\n",
    "print(\"X_test_scaled shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
